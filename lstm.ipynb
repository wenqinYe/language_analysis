{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tag.stanford import StanfordPOSTagger as tagger\n",
    "import treetaggerwrapper\n",
    "from sklearn.manifold import TSNE\n",
    "import pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class Data():\n",
    "    \"\"\"\n",
    "    The data class holds numerical sequence batch data (Example: [[1, 2, 3, 4], [1, 2, 3, 1]] ) and \n",
    "    provides easy operations on the data to be used in a machine learning model. \n",
    "    \"\"\"\n",
    "    def __init__(self, unique_characters=[], data=[], shuffle=False):\n",
    "        self.unique_characters = unique_characters\n",
    "        self.unique_characters.insert(0, u\"<EOS>\") \n",
    "        self.unique_characters.insert(0, u\"<GO>\") \n",
    "        self.unique_characters.insert(0, u\"<PAD>\") \n",
    "        \n",
    "        self.num_unique_chars = len(self.unique_characters)\n",
    "        \n",
    "        self.char2index = {v: i for i, v in enumerate(self.unique_characters)}\n",
    "        self.index2char = {i: v for i, v in enumerate(self.unique_characters)}\n",
    "        \n",
    "        self.embedding_size = 5\n",
    "        self.embeddings = tf.Variable(tf.random_uniform([len(self.char2index), self.embedding_size], -1.0, 1.0))\n",
    "        \n",
    "        self.current_index = 0\n",
    "        \n",
    "        self.data = np.array(data)\n",
    "        self.length = len(self.data)\n",
    "        self.permuted_indices = np.random.permutation(self.length)\n",
    "        \n",
    "        self.shuffle = shuffle\n",
    "        \n",
    "        \n",
    "    def to_one_hot(self, batches, as_tf_tensor=False):\n",
    "        \"\"\"\n",
    "        Takes data of the shape [batch, numbers]\n",
    "        and converts it into a one hot representation\n",
    "        to be used by a LSTM or other neural network\n",
    "        Example: \n",
    "                [\n",
    "                    [0, 1],\n",
    "                    [0, 2]\n",
    "                ]\n",
    "                Converts into one-hot representation ->\n",
    "                [\n",
    "                    [[1, 0, 0], [0, 1, 0]],\n",
    "                    [[1, 0, 0], [0, 0, 1]]\n",
    "                ]\n",
    "        \n",
    "        Params:\n",
    "            batches: Batch of character input data\n",
    "            as_tensor: If the one_hot representation should be a np array or \n",
    "                       a tensorflow tensor.\n",
    "            \n",
    "        Returns:\n",
    "            A one hot tensor representation of the input batch data\n",
    "        \"\"\" \n",
    "        one_hot = tf.one_hot(batches, depth=self.num_unique_chars)\n",
    "        if not as_tf_tensor:\n",
    "            one_hot = np.zeros(one_hot.shape)\n",
    "            for batch_index, batch in enumerate(batches):\n",
    "                for index, one_hot_index in enumerate(batch):\n",
    "                    one_hot[batch_index, index, one_hot_index] = 1\n",
    "        return one_hot \n",
    "    \n",
    "    def embedding(self, batch):\n",
    "        \"\"\"\n",
    "        Takes a batch of data represented by [batch, numbers] and outputs the embeddings\n",
    "        Example: \n",
    "            [[1, 2, 3], [3, 4, 1]] ->\n",
    "            [\n",
    "                [[0.3, 0.27, 0.13], [0.58, 0.27, 0.23] [0.3, 0.27, -0.03]],\n",
    "                [[0.17, 0.18, 0.13], [0.28, -0.44, 0.17] [0.3, 0.24, -0.03]]\n",
    "            ]\n",
    "            \n",
    "        Params:\n",
    "            batch: A [batch, numbers] vector with the numbers in each row representing the embedding id\n",
    "            \n",
    "        Returns:\n",
    "            A [batch_size, sequence_length, embedding_size] embedding of the data\n",
    "        \"\"\"\n",
    "        return tf.nn.embedding_lookup(self.embeddings, batch) \n",
    "    \n",
    "    def next(self, number):\n",
    "        \"\"\"\n",
    "        Returns a batch from the provided data held by this class.\n",
    "        This function automatically zero pads the data so that all sequences\n",
    "        are the same length.\n",
    "        \n",
    "        Params:\n",
    "            number: The number of samples to retrieve from the data to be included in the batch\n",
    "            \n",
    "        Returns:\n",
    "            A batch of data with the number of elements.\n",
    "            Example: next(3) would return ->\n",
    "                     [[1, 3, 2, 0, 0],\n",
    "                      [1, 2, 3, 1, 0],\n",
    "                      [1, 1, 0, 0, 0]]\n",
    "        \"\"\"\n",
    "        if self.shuffle:\n",
    "            batch_indices = self.permuted_indices[self.current_index: self.current_index+number]\n",
    "            batch = self.data[batch_indices]\n",
    "        else:\n",
    "            batch = self.data[self.current_index: self.current_index+number]\n",
    "        \n",
    "        max = 0\n",
    "        for item in batch:\n",
    "            if len(item) > max:\n",
    "                max = len(item)\n",
    "                \n",
    "        padded_batch = np.zeros((len(batch), max))\n",
    "        \n",
    "        for idx, item in enumerate(batch):\n",
    "            padded_batch[idx, np.arange(len(item))] = item\n",
    "            \n",
    "        if self.current_index + number >= len(self.data):\n",
    "            self.current_index = 0\n",
    "        else:\n",
    "            self.current_index += number\n",
    "        \n",
    "        return padded_batch\n",
    "    \n",
    "    def set_data(self, data, sort_length=False):\n",
    "        \"\"\"\n",
    "        INTERNAL: When new data is set we wish to update their related variables. \n",
    "        \"\"\"\n",
    "        self.data = np.array(data)\n",
    "        self.length = len(data)\n",
    "        self.permuted_indices = np.random.permutation(self.length)\n",
    "        \n",
    "    \n",
    "def french_to_universal(french_pos_tagged):\n",
    "    \"\"\"\n",
    "    This function takes a tagged french text corpus and converts it into a universal part of speech tag. \n",
    "    \n",
    "    French POS to Universal Mapping credit to:\n",
    "    https://stackoverflow.com/questions/27513185/simplifying-the-french-pos-tag-set-with-nltk\n",
    "    \n",
    "    Params:\n",
    "        french_pos_tagged: An array of tagged tuples for a text corpus. Example: [('je', 'PRON'), ('veux', 'V')]\n",
    "    Returns:\n",
    "        A universal tag of the provided french text corpus\n",
    "    \"\"\"\n",
    "    french_to_universal = {\n",
    "        u\"ADJ\": u\"ADJ\",\n",
    "        u\"ADJWH\": u\"ADJ\",\n",
    "        u\"ADV\": u\"ADV\",\n",
    "        u\"ADVWH\":u\"ADV\",\n",
    "        u\"CC\": u\"CONJ\",    \n",
    "        u\"CLO\":u\"PRON\",\n",
    "        u\"CLR\": u\"PRON\",\n",
    "        u\"CLS\": u\"PRON\",\n",
    "        u\"CS\": u\"CONJ\",\n",
    "        u\"DET\": u\"DET\",\n",
    "        u\"DETWH\": u\"DET\",\n",
    "        u\"ET\": u\"X\",\n",
    "        u\"NC\": u\"NOUN\",\n",
    "        u\"NPP\": u\"NOUN\",\n",
    "        u\"P\": u\"ADP\",\n",
    "        u\"PUNC\": u\".\",\n",
    "        u\"PRO\": u\"PRON\",\n",
    "        u\"PROREL\": u\"PRON\",\n",
    "        u\"PROWH\": u\"PRON\",\n",
    "        u\"V\": u\"VERB\",\n",
    "        u\"VIMP\": u\"VERB\",\n",
    "        u\"VINF\": u\"VERB\",\n",
    "        u\"VPP\": u\"VERB\",\n",
    "        u\"VPR\": u\"VERB\",\n",
    "        u\"VS\": u\"VERB\",\n",
    "        u\"N\": u\"NOUN\",\n",
    "        u\"I\": u\"X\",\n",
    "        u\"PREF\": u\"PRT\",\n",
    "        u\"C\": u\"CONJ\",\n",
    "        u\"CL\": u\"PRON\",\n",
    "    }\n",
    "    \n",
    "    return [(word, french_to_universal[tag]) for word, tag in french_pos_tagged]\n",
    "\n",
    "def english_to_universal(english_pos_tagged):\n",
    "    \"\"\"\n",
    "    This function takes a tagged text english corpus and converts it into a universal part of speech tag. \n",
    "    \n",
    "    Params:\n",
    "        english_pos_tagged: An array of tagged tuples. Example: [('I', 'PR'), ('like', 'VB'), ('pizza', N)]\n",
    "    Returns: \n",
    "        \n",
    "    \"\"\"\n",
    "    return [(word, nltk.tag.map_tag('en-ptb', 'universal', pos)) for word, pos in english_pos_tagged]\n",
    "\n",
    "french_data = Data(unique_characters=[u'ADV', u'NOUN', u'ADP', u'PRT', u'DET', u'.', u'PRON', u'VERB', u'X', u'NUM', u'CONJ', u'ADJ'])\n",
    "english_data = Data(unique_characters=[u'ADV', u'NOUN', u'ADP', u'PRT', u'DET', u'.', u'PRON', u'VERB', u'X', u'NUM', u'CONJ', u'ADJ'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take an english and french text corpus, and tag the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Replace JAR path with your stanford pos tagger jar path\n",
    "#You can find it https://nlp.stanford.edu/software/tagger.shtml\n",
    "jar_path = \"/Users/wenqin/Documents/datasets/stanford-postagger-2017/stanford-postagger-3.8.0.jar\"\n",
    "\n",
    "french_tagger = tagger(\"/Users/wenqin/Documents/datasets/stanford-postagger-2017/models/french.tagger\", path_to_jar=jar_path)\n",
    "\n",
    "base_path = \"/Users/wenqin/Documents/datasets/texts/\"\n",
    "\n",
    "french_file = open(base_path + \"french/book1.txt\").read()\n",
    "french_tagged = french_tagger.tag(french_file.split(\" \"))\n",
    "french_tagged = french_to_universal(french_tagged)\n",
    "\n",
    "english_file = open(base_path + \"english/pride_and_prejudice.txt\").read()\n",
    "english_tagged = nltk.pos_tag(nltk.word_tokenize(unicode(english_file, 'utf-8')))\n",
    "english_tagged = english_to_universal(english_tagged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the POS tags to a numerical representation so they can be used by a neural network.\n",
    "The goal is to convert something like \n",
    "[('he', u'PRON'), ('gently', u'ADV'), ('patted', u'VERB'), ('her', u'PRON'), ('on', u'ADP'), ('the', u'DET'), ('back', u'NOUN')] \n",
    "into \n",
    "[1, 5, 2, 3, 7, 8, 9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def numerical_representation(tagged_words, data_class, lang='fr', sort_by_length=True):\n",
    "    \"\"\"\n",
    "    Converts a tagged corpus into a numerical batch representation. \n",
    "    Also splis the text corpus into individual sentences.\n",
    "    \n",
    "    params:\n",
    "        tagged_words: The text word corpus\n",
    "        data_class: The data class that contains the indices for each POS tag\n",
    "        lang: The language\n",
    "        sort_by_length: Sort the sentences in the batch by sentence length. This is useful for \n",
    "                        LSTM training.\n",
    "                        \n",
    "    returns:\n",
    "        Returns a numerical batch representation of the POS tags.\n",
    "    \"\"\"\n",
    "    sentences_tags = [[]]\n",
    "    \n",
    "    for word, tag in tagged_words:\n",
    "        sentences_tags[-1].append((word, tag))\n",
    "        if lang == 'fr':\n",
    "            if word[-1] == \".\":\n",
    "                sentences_tags[-1].append((u\".\", u\".\"))\n",
    "                sentences_tags.append([])\n",
    "        else:\n",
    "            if word == \".\":\n",
    "                sentences_tags.append([])\n",
    "    \n",
    "    data = []\n",
    "    for sentence_tags in sentences_tags:\n",
    "        data.append([data_class.char2index[tag] for word, tag in sentence_tags])\n",
    "        \n",
    "    return sorted(data, key=len)\n",
    "\n",
    "english_data.set_data(numerical_representation(french_tagged, english_data))\n",
    "french_data.set_data(numerical_representation(french_tagged, french_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class LSTM_Cell:\n",
    "    def __init__(self, time_steps=None, input_size=None, cell_size=None, learning_rate=1e-3):\n",
    "        assert cell_size is not None and input_size is not None\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.cell_size = cell_size\n",
    "        \n",
    "        self.inputs = tf.placeholder(tf.float32, shape=[None, None, input_size], name=\"lstm_inputs\")\n",
    "        self.targets = tf.placeholder(tf.float32, shape=[None, None, input_size], name=\"lstm_targets\")\n",
    "        \n",
    "        self.w_f = tf.Variable(tf.random_normal(shape=[self.input_size, self.cell_size]), name=\"w_f\")\n",
    "        self.u_f = tf.Variable(tf.random_normal(shape=[self.cell_size, self.cell_size]), name=\"u_f\")\n",
    "        self.b_f = tf.Variable(tf.constant(0., shape=[self.cell_size]), name=\"b_f\")\n",
    "\n",
    "        self.w_i = tf.Variable(tf.random_normal(shape=[self.input_size, self.cell_size]), name=\"w_i\")\n",
    "        self.u_i = tf.Variable(tf.random_normal(shape=[self.cell_size, self.cell_size]), name=\"u_i\")\n",
    "        self.b_i = tf.Variable(tf.constant(0., shape=[self.cell_size]), name=\"b_i\")\n",
    "\n",
    "        self.w_o = tf.Variable(tf.random_normal(shape=[self.input_size, self.cell_size]), name=\"w_o\")\n",
    "        self.u_o = tf.Variable(tf.random_normal(shape=[self.cell_size, self.cell_size]), name=\"u_o\")\n",
    "        self.b_o = tf.Variable(tf.constant(0., shape=[self.cell_size]), name=\"b_0\")\n",
    "        \n",
    "        self.w_c = tf.Variable(tf.random_normal(shape=[self.input_size, self.cell_size]), name=\"w_c\")\n",
    "        self.u_c = tf.Variable(tf.random_normal(shape=[self.cell_size, self.cell_size]), name=\"u_cs\")\n",
    "        self.b_c = tf.Variable(tf.constant(0., shape=[self.cell_size]), name=\"b_c\")\n",
    "        \n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        self.outputs = None\n",
    "        self.last_hidden_state = None\n",
    "        self.last_cell_state = None\n",
    "        \n",
    "        self.time_steps = time_steps\n",
    "        \n",
    "    def call(self, state_tuple, x):\n",
    "        \"\"\"\n",
    "        One iteration of the LSTM cell. \n",
    "        \n",
    "        params:\n",
    "            state_tuple: The previous hidden and cell state (of the shape [hidden or cell, batch_size])\n",
    "            x: The batch input into the neural network (of the shape [batch_size, input_size])\n",
    "               Example: [[3, 2, 1], [1, 2, 3]]  -> Where each row represents a item in the batch\n",
    "        returns:\n",
    "            A new state tuple representing the new hidden and cel state\n",
    "        \"\"\"\n",
    "        previous_hidden, previous_cell_state = tf.unstack(state_tuple)\n",
    "        \n",
    "        f = tf.nn.sigmoid(tf.matmul(x, self.w_f) + tf.matmul(previous_hidden, self.u_f) + self.b_f)\n",
    "        i = tf.nn.sigmoid(tf.matmul(x, self.w_i) + tf.matmul(previous_hidden, self.u_i) + self.b_i)\n",
    "        o = tf.nn.sigmoid(tf.matmul(x, self.w_o) + tf.matmul(previous_hidden, self.u_o) + self.b_o)\n",
    "        \n",
    "        cell_state_additions = tf.nn.tanh(tf.matmul(x, self.w_c) + tf.matmul(x, self.w_c) + self.b_c)\n",
    "        \n",
    "        cell_state = tf.multiply(cell_state_additions, i) + tf.multiply(previous_cell_state, f)\n",
    "        new_hidden = tf.nn.tanh(tf.multiply(cell_state, o))\n",
    "        \n",
    "        return tf.stack([new_hidden, cell_state])\n",
    "    \n",
    "    def dynamic_rnn(self, input_sequence=None, dynamic_output=False, initial_state_tuple=None):\n",
    "        \"\"\"\n",
    "        Given a batch major input sequence it will run convert it to time major and run through\n",
    "        the LSTM cell step by step.\n",
    "        \n",
    "        params:\n",
    "            input_sequence: A batch major input sequence (of the shape [batch_size, sequence_length, input_size])\n",
    "            initial_state_tuple: A state tuple that should be used otherwise a zero one will be used.\n",
    "            \n",
    "        returns:\n",
    "            The hidden states and cell states for each batch item for each time step (returned batch_major and \n",
    "            not time major).\n",
    "        \"\"\"\n",
    "        input_sequence = tf.transpose(input_sequence, [1, 0, 2]) #tranpose to allow for batch processing    \n",
    "        \n",
    "        batch_items = tf.shape(input_sequence)[1]\n",
    "        \n",
    "        if initial_state_tuple is None:\n",
    "            initial_state_tuple = self.initial_state_tuple(batch_items)\n",
    "        \n",
    "        state_tuples = tf.scan(self.call, input_sequence, initializer=initial_state_tuple)\n",
    "        \n",
    "        hidden_states, cell_states = self.split_state_tuples(state_tuples)\n",
    "        last_hidden_states, last_cell_states = tf.unstack(tf.gather(state_tuples, tf.shape(state_tuples)[1]))\n",
    "        \n",
    "        return hidden_states, cell_states, tf.stack([last_hidden_states, last_cell_states])\n",
    "    \n",
    "    def dynamic_output_run(self, state_tuple=None, current_time_step=1, history=[]):\n",
    "        \"\"\"\n",
    "        Similar to dynamic_rnn expect the output of one step is used ad the input to the next\n",
    "        step. \n",
    "        \n",
    "        params:\n",
    "            state_tuple: An initial state tuple\n",
    "            current_time_step: The current time step for creating the computational graph (INTERNAL)\n",
    "            history: The state tupel history (INTERNAL)\n",
    "            \n",
    "        returns:\n",
    "            A batch major array with the hidden states at each time step. \n",
    "        \"\"\"\n",
    "        assert state_tuple is not None\n",
    " \n",
    "        hidden_state, cell_state = tf.unstack(state_tuple)\n",
    "        new_state_tuple = self.call(state_tuple, hidden_state)\n",
    "        new_hidden_state, new_cell_state = tf.unstack(new_state_tuple)\n",
    "        history.append(new_hidden_state)\n",
    "        \n",
    "        if current_time_step < self.time_steps:\n",
    "             self.dynamic_output_run(new_state_tuple, current_time_step=current_time_step+1, history=history)\n",
    "        \n",
    "        return tf.transpose(tf.stack(history, name='dynamic_output_run'), [1, 0, 2])\n",
    "        \n",
    "    def split_state_tuples(self, state_tuples):\n",
    "        \"\"\"\n",
    "        Splits time major batch tuples. \n",
    "        \n",
    "        params:\n",
    "            state_tuples: time major state tuple\n",
    "            \n",
    "        returns:\n",
    "            Batch major hidden states and cell states\n",
    "        \"\"\"\n",
    "        hidden_states, cell_states = tf.split(state_tuples, 2, axis=1)\n",
    "        #permute the shapes to make the data batch major instead of time major\n",
    "        \n",
    "        hidden_states = tf.squeeze(hidden_states, axis=1) #axis-1 is the now irrelevant state-tuple axis\n",
    "        cell_states = tf.squeeze(cell_states, axis=1)     #axis-1 is the now irrelevant state-tuple axis\n",
    "        \n",
    "        hidden_states = tf.transpose(hidden_states, [1, 0, 2])\n",
    "        cell_states = tf.transpose(cell_states, [1, 0, 2]) \n",
    "        \n",
    "        return hidden_states, cell_states\n",
    "    \n",
    "    def initial_state_tuple(self, batch_items):\n",
    "        initial_hidden = tf.zeros([batch_items, self.cell_size], name='hidden')\n",
    "        initial_cell_state = tf.zeros([batch_items, self.cell_size], name='cell_state')\n",
    "        initial_state_tuple = tf.stack([initial_hidden, initial_cell_state])\n",
    "        \n",
    "        return initial_state_tuple\n",
    "    \n",
    "    @property\n",
    "    def loss(self):\n",
    "        \"\"\"\n",
    "        Takes the input and feeds it in sequentially into the cell. The output is then compared\n",
    "        to the targets to compute a loss.\n",
    "        \n",
    "        returns:\n",
    "            A loss value for the inputs and targets\n",
    "        \"\"\"\n",
    "        hidden_states, cell_states, last_state_tuples = self.dynamic_rnn(self.inputs)\n",
    "        self.outputs = [hidden_states, cell_states]\n",
    "    \n",
    "        self.last_hidden_state, self.last_cell_state = tf.unstack(last_state_tuples)\n",
    "        \n",
    "        return tf.reduce_mean(tf.squared_difference(self.targets, hidden_states))\n",
    "    \n",
    "class Optimizer:\n",
    "    def __init__(self, model, learning_rate=1e-2):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.model = model\n",
    "        self.optimizer = tf.train.RMSPropOptimizer(self.learning_rate).minimize(model.loss)\n",
    "        \n",
    "        self.sess = tf.InteractiveSession()\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "    def run_training(self, inputs, targets, epochs=10, print_interval=10):\n",
    "        np.set_printoptions(suppress=True)\n",
    "        for i in range(epochs):\n",
    "            if print_interval is not None and i % print_interval == 0:\n",
    "                print self.sess.run(self.model.predictions, {self.model.inputs: inputs, self.model.targets: targets})\n",
    "                print \"-\" * 50\n",
    "            self.sess.run(self.optimizer, {self.model.inputs: inputs, self.model.targets: targets})\n",
    "            \n",
    "    def plot_cells(self):\n",
    "        cell_state = self.sess.run(self.c)\n",
    "        print cell_state\n",
    "        plt.matshow(cell_state, cmap='Blues'), plt.show()\n",
    "        \n",
    "class Seq2SeqModel:\n",
    "    def __init__(self, data=english_data):\n",
    "        self.encoder_cell = LSTM_Cell(time_steps=3, input_size=5, cell_size=10)\n",
    "        self.decoder_cell = LSTM_Cell(time_steps=3, input_size=5, cell_size=10)\n",
    "        \n",
    "        self.data = data\n",
    "        \n",
    "        self.inputs = tf.placeholder(dtype=tf.int32, shape=(None, None))\n",
    "        self.encoder_cell.inputs = data.embedding(self.inputs)\n",
    "        self.targets = tf.placeholder(dtype=tf.float32, shape=(None, None, 15))\n",
    "        \n",
    "        self.hidden_states, self.cell_states, self.last_state_tuple = self.encoder_cell.dynamic_rnn(self.encoder_cell.inputs)\n",
    "        #self.outputs = self.decoder_cell.dynamic_output_run(state_tuple=self.last_state_tuple, history=[])\n",
    "        self.outputs, self.cell_states, _ = self.decoder_cell.dynamic_rnn(self.encoder_cell.inputs, initial_state_tuple=self.last_state_tuple)\n",
    "        \n",
    "        self.predictions = tf.layers.dense(self.hidden_states, 15, activation=tf.nn.tanh)\n",
    "        \n",
    "        self.loss = tf.reduce_mean(tf.squared_difference(self.targets, self.predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Running the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.06292582  0.38080144 -0.34114242  0.18521166  0.26886749]\n",
      " [-0.74775457 -0.20024681 -0.43615842  0.9684844   0.37466073]\n",
      " [-0.47493839  0.66137385  0.47926116 -0.33175564 -0.88810873]\n",
      " [ 0.82891178  0.58622289 -0.04501843  0.22978473 -0.15813279]\n",
      " [ 0.54829597  0.0303216   0.08871555 -0.73703074 -0.63573098]\n",
      " [-0.99268031  0.62409973  0.29787898 -0.01935101 -0.13118982]\n",
      " [ 0.02516007 -0.94336915  0.033952   -0.40571928  0.64658642]\n",
      " [-0.55931687  0.3674612   0.38335252  0.18864441  0.54403615]\n",
      " [ 0.95477581  0.61956382 -0.24071288 -0.50270104  0.62175822]\n",
      " [ 0.46419525 -0.20735908 -0.79318094  0.14713621  0.08648539]\n",
      " [ 0.43564034  0.90195942 -0.17654634  0.2228663  -0.05406189]\n",
      " [-0.10844874  0.35895514 -0.8660531   0.3250463  -0.68415904]\n",
      " [-0.72741151  0.37349296 -0.31708527 -0.69286013 -0.14156079]\n",
      " [-0.00282192  0.49249101 -0.3582685   0.85514593 -0.67705035]\n",
      " [ 0.09741592 -0.76620865  0.94971442  0.96263194  0.63756943]]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "[array([[[ 0.03775339,  0.00833569, -0.00836269,  0.01234768,  0.01838474,\n",
      "          0.01145815,  0.01025583,  0.01411091, -0.01486028,  0.95457929,\n",
      "         -0.05430296, -0.04444964, -0.0084267 , -0.03856189,  0.02579312],\n",
      "        [ 0.02027752,  0.0048759 , -0.00490276,  0.02861909,  0.03735387,\n",
      "          0.01838023,  0.0027973 ,  0.00533731, -0.03587342, -0.03197947,\n",
      "          0.96685827, -0.01244627, -0.00496648, -0.02868504,  0.03083917],\n",
      "        [-0.00199148,  0.00361353, -0.00360471,  0.94402045,  0.03523726,\n",
      "          0.00340572,  0.00227041, -0.01940948,  0.00543729, -0.01855002,\n",
      "         -0.03444332,  0.00052333, -0.00358367, -0.01937643,  0.04870165],\n",
      "        [-0.00091003, -0.00077094,  0.00077554,  0.02331883,  0.04344243,\n",
      "          0.00280947, -0.00069439,  0.97273612, -0.0256802 , -0.01383735,\n",
      "          0.0370977 , -0.01424627,  0.00078652, -0.01174333,  0.00098155],\n",
      "        [ 0.03560884,  0.0048269 , -0.0048623 , -0.00766435,  0.01760057,\n",
      "          0.00349709,  0.00265092,  0.01121355,  0.01226598, -0.01053899,\n",
      "          0.96563023, -0.02056541, -0.00494633, -0.01209561,  0.01190919],\n",
      "        [ 0.01653011,  0.01179229, -0.01181898,  0.00334507,  0.01271264,\n",
      "          0.98309517,  0.01083717, -0.01138855, -0.01065431, -0.00745149,\n",
      "         -0.01140135, -0.01411874, -0.01188237, -0.00796028,  0.01160282],\n",
      "        [ 0.00937816, -0.00080883,  0.00081207,  0.0080202 ,  0.04395408,\n",
      "          0.0174807 , -0.00080352,  0.97391367, -0.04210443, -0.01795319,\n",
      "          0.02865025, -0.01027014,  0.00081982, -0.01646967, -0.00025757],\n",
      "        [ 0.0295522 ,  0.02040097, -0.02043892,  0.01987221,  0.98529965,\n",
      "          0.01882868,  0.01797647, -0.03870671, -0.0192358 , -0.01717974,\n",
      "         -0.01395698, -0.02932488, -0.02052886, -0.02787682,  0.02586082],\n",
      "        [ 0.01361809,  0.01177879, -0.01180565,  0.00621724,  0.01407284,\n",
      "          0.9829098 ,  0.0108254 , -0.00795348, -0.00694147, -0.00364397,\n",
      "         -0.01027312, -0.0164676 , -0.01186944, -0.00512023,  0.00455897],\n",
      "        [ 0.02496567,  0.02029759, -0.02033222,  0.02265261,  0.98494291,\n",
      "          0.01714155,  0.01802473, -0.03432141, -0.01476803, -0.01847292,\n",
      "         -0.01762957, -0.02578172, -0.02041424, -0.02705517,  0.02903545],\n",
      "        [ 0.01365748,  0.01177946, -0.01180631,  0.00625385,  0.01402606,\n",
      "          0.98291129,  0.01082652, -0.00821262, -0.00687252, -0.00363581,\n",
      "         -0.01033047, -0.01643381, -0.0118701 , -0.00513993,  0.00463609],\n",
      "        [ 0.02496534,  0.0202974 , -0.02033202,  0.02266355,  0.98494178,\n",
      "          0.01714434,  0.01802464, -0.03432617, -0.01473809, -0.01848412,\n",
      "         -0.01760938, -0.02578048, -0.02041404, -0.02706279,  0.02902249],\n",
      "        [ 0.01365822,  0.01177947, -0.01180632,  0.00625386,  0.01402654,\n",
      "          0.98291117,  0.0108265 , -0.00821292, -0.00687281, -0.00363602,\n",
      "         -0.01033155, -0.01643432, -0.01187011, -0.00514011,  0.00463706],\n",
      "        [ 0.00850137, -0.00080877,  0.00081241,  0.00834468,  0.04450091,\n",
      "          0.01671988, -0.00078957,  0.97392368, -0.04220703, -0.01819353,\n",
      "          0.02870867, -0.01002969,  0.00082111, -0.01669676,  0.00069512],\n",
      "        [ 0.02950482,  0.02040152, -0.02043942,  0.0198092 ,  0.98530227,\n",
      "          0.0188263 ,  0.01797876, -0.03868722, -0.01923914, -0.01716484,\n",
      "         -0.01400302, -0.02927078, -0.02052926, -0.02792819,  0.02588832],\n",
      "        [ 0.00891215, -0.0005768 ,  0.0005476 ,  0.01777955,  0.02206197,\n",
      "         -0.00430909,  0.00090586,  0.01067532, -0.00338416,  0.0021868 ,\n",
      "         -0.03788147, -0.02184177,  0.00047787,  0.95632106,  0.00972439],\n",
      "        [ 0.00846211,  0.01172106, -0.01174411,  0.01530106,  0.01537944,\n",
      "          0.98284644,  0.01096079, -0.00788547, -0.00055921, -0.00352477,\n",
      "         -0.02575933, -0.01214882, -0.01179886,  0.00569346, -0.00985284],\n",
      "        [ 0.00951951, -0.00080687,  0.00080924,  0.0073299 ,  0.04333826,\n",
      "          0.01679627, -0.00084039,  0.97385436, -0.04318321, -0.01664616,\n",
      "          0.03103717, -0.01190612,  0.00081493, -0.01502612,  0.00144122],\n",
      "        [ 0.02966189,  0.02040015, -0.02043818,  0.01987254,  0.98529434,\n",
      "          0.01884328,  0.01797206, -0.03880866, -0.01937567, -0.01703112,\n",
      "         -0.01383423, -0.02942842, -0.02052833, -0.02772492,  0.02585145],\n",
      "        [ 0.01361728,  0.01177878, -0.01180564,  0.00621704,  0.01407289,\n",
      "          0.98290974,  0.01082539, -0.00795139, -0.00694162, -0.00364379,\n",
      "         -0.01027277, -0.01646775, -0.01186943, -0.00511977,  0.00455988],\n",
      "        [ 0.00849639, -0.00080876,  0.00081241,  0.00834781,  0.04452524,\n",
      "          0.01672002, -0.00078921,  0.9739238 , -0.04223176, -0.01818999,\n",
      "          0.028706  , -0.01001292,  0.00082114, -0.01669445,  0.00067034],\n",
      "        [ 0.02950533,  0.02040152, -0.02043943,  0.01980832,  0.98530233,\n",
      "          0.01882655,  0.01797875, -0.03868686, -0.01924027, -0.01716335,\n",
      "         -0.01400465, -0.0292714 , -0.02052927, -0.02792771,  0.02588916],\n",
      "        [ 0.013617  ,  0.01177878, -0.01180563,  0.00621749,  0.01407123,\n",
      "          0.98290986,  0.01082544, -0.00795517, -0.00694061, -0.00364385,\n",
      "         -0.01027092, -0.01646569, -0.01186942, -0.00512043,  0.00455694],\n",
      "        [ 0.00849629, -0.00080877,  0.00081242,  0.00834814,  0.04452487,\n",
      "          0.01672016, -0.00078921,  0.9739238 , -0.04223134, -0.01819028,\n",
      "          0.02870573, -0.01001298,  0.00082114, -0.01669478,  0.00067038],\n",
      "        [ 0.0295053 ,  0.02040152, -0.02043943,  0.01980831,  0.98530233,\n",
      "          0.01882654,  0.01797875, -0.03868689, -0.01924021, -0.01716341,\n",
      "         -0.01400461, -0.02927131, -0.02052927, -0.02792777,  0.02588912],\n",
      "        [ 0.01486251,  0.00489   , -0.00491165,  0.01926216,  0.04200134,\n",
      "          0.01682137,  0.003012  , -0.00185296, -0.0431399 , -0.03140864,\n",
      "          0.96803087, -0.00477289, -0.00496294, -0.0317483 ,  0.03187698],\n",
      "        [ 0.00726533, -0.000822  ,  0.00082664,  0.01294817,  0.0411718 ,\n",
      "          0.01687969, -0.00074178,  0.97364062, -0.03417787, -0.01985131,\n",
      "          0.03093725, -0.00879557,  0.0008377 , -0.01601269, -0.00752686],\n",
      "        [ 0.014548  ,  0.0070254 , -0.00703393,  0.0012283 ,  0.00362034,\n",
      "          0.0114757 ,  0.00613791, -0.00897696, -0.0059069 , -0.01298939,\n",
      "         -0.00446507, -0.01155927, -0.0070542 , -0.01392799,  0.9654581 ],\n",
      "        [ 0.0300507 ,  0.02036288, -0.02040116,  0.02837751,  0.98507196,\n",
      "          0.01892193,  0.01791328, -0.03703247, -0.01656354, -0.02017546,\n",
      "         -0.01524578, -0.02997156, -0.02049187, -0.03326578,  0.02881672],\n",
      "        [ 0.01390698,  0.01177914, -0.01180604,  0.00615756,  0.0142503 ,\n",
      "          0.98289257,  0.01082447, -0.00807795, -0.00706139, -0.00369541,\n",
      "         -0.01066223, -0.01635778, -0.01186991, -0.0052267 ,  0.00510638],\n",
      "        [ 0.02497535,  0.02029743, -0.02033206,  0.02266753,  0.98494172,\n",
      "          0.01714532,  0.01802433, -0.03432105, -0.01473946, -0.01848091,\n",
      "         -0.01763231, -0.02579173, -0.0204141 , -0.02705297,  0.02902799],\n",
      "        [-0.00859153,  0.00367338, -0.00366084,  0.94465572,  0.05189694,\n",
      "         -0.00157137,  0.00250664, -0.01156678,  0.01939591, -0.02235565,\n",
      "         -0.04361735,  0.00363468, -0.00363094, -0.02312678,  0.02961523],\n",
      "        [ 0.02778554,  0.00505273, -0.0050805 ,  0.00672415,  0.05578865,\n",
      "          0.01930437,  0.0030168 , -0.00798801, -0.01880814, -0.03537192,\n",
      "          0.96714717, -0.00881482, -0.00514636, -0.02987891,  0.01592566],\n",
      "        [ 0.02659194,  0.02033488, -0.02037071,  0.0370397 ,  0.98524225,\n",
      "          0.01809964,  0.01794613, -0.03801301, -0.02112873, -0.03361326,\n",
      "         -0.01208467, -0.02698565, -0.02045561, -0.02956888,  0.02795019],\n",
      "        [ 0.01355644,  0.01177764, -0.01180442,  0.00616295,  0.0140381 ,\n",
      "          0.98289484,  0.01082828, -0.00808001, -0.00686879, -0.00352372,\n",
      "         -0.0101787 , -0.01633892, -0.01186801, -0.00512747,  0.00469043],\n",
      "        [ 0.02495828,  0.02029722, -0.02033184,  0.02268755,  0.98494118,\n",
      "          0.01714409,  0.01802468, -0.03430819, -0.01474521, -0.01848708,\n",
      "         -0.01763663, -0.02577306, -0.02041384, -0.02707354,  0.02903804],\n",
      "        [ 0.01155636,  0.00705862, -0.00706332,  0.00305601,  0.01186752,\n",
      "          0.01173302,  0.0063461 , -0.01933149, -0.00766909, -0.00748576,\n",
      "         -0.00270919, -0.00534625, -0.00707449, -0.01099931,  0.96437538],\n",
      "        [ 0.01712941,  0.00698437, -0.00699069,  0.0014295 ,  0.00664478,\n",
      "          0.01294579,  0.00622157, -0.01386711, -0.00284129, -0.02220364,\n",
      "         -0.01291245, -0.00507255, -0.00700572, -0.00300719,  0.9646607 ],\n",
      "        [ 0.01414506,  0.01177315, -0.01179934,  0.00515817,  0.0144822 ,\n",
      "          0.98297131,  0.01084577, -0.00628463, -0.00884313, -0.00423879,\n",
      "         -0.01009168, -0.01471255, -0.01186153, -0.00643224,  0.00241208],\n",
      "        [ 0.02545429,  0.02030192, -0.02033696,  0.0231065 ,  0.98495179,\n",
      "          0.01721187,  0.01801082, -0.03436332, -0.01466792, -0.01866976,\n",
      "         -0.01780855, -0.02632001, -0.02041997, -0.02716667,  0.02896331],\n",
      "        [ 0.01367352,  0.01177964, -0.01180651,  0.00624973,  0.01404455,\n",
      "          0.98291117,  0.01082602, -0.00819671, -0.00688399, -0.00363771,\n",
      "         -0.01035475, -0.01645401, -0.01187033, -0.00514138,  0.00466569],\n",
      "        [ 0.02496623,  0.02029741, -0.02033203,  0.02266382,  0.98494178,\n",
      "          0.01714427,  0.01802461, -0.03432545, -0.01473827, -0.01848374,\n",
      "         -0.01761162, -0.02578178, -0.02041405, -0.02706172,  0.02902293],\n",
      "        [ 0.01155621,  0.00705862, -0.00706332,  0.00305647,  0.01186717,\n",
      "          0.01173186,  0.00634609, -0.01933021, -0.00766909, -0.00748483,\n",
      "         -0.00270908, -0.00534634, -0.00707449, -0.01100049,  0.96437538],\n",
      "        [ 0.01391976,  0.01176966, -0.01179567,  0.00517472,  0.01417892,\n",
      "          0.98296857,  0.01085026, -0.0066072 , -0.00865751, -0.00448286,\n",
      "         -0.00959017, -0.0144825 , -0.01185744, -0.00614623,  0.0021704 ],\n",
      "        [ 0.02545778,  0.02030219, -0.02033723,  0.02311006,  0.98495287,\n",
      "          0.01721136,  0.01801081, -0.03438862, -0.01469247, -0.01867843,\n",
      "         -0.01776551, -0.02632337, -0.02042025, -0.02717947,  0.0289643 ],\n",
      "        [ 0.01179986,  0.00845648, -0.00846226,  0.00690282,  0.01032962,\n",
      "          0.01611226,  0.0133021 , -0.0321517 ,  0.94434309, -0.00963494,\n",
      "          0.00242099, -0.01620061, -0.00847597, -0.01156819,  0.00892095]]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "model = Seq2SeqModel(data=english_data)\n",
    "optimizer = Optimizer(model)\n",
    "print optimizer.sess.run(english_data.embeddings)\n",
    "for epoch in range(10):\n",
    "    print epoch\n",
    "    for _ in range(english_data.length/16):\n",
    "        input_sequence = english_data.next(16).astype(\"int\")\n",
    "        target_sequence = english_data.to_one_hot(input_sequence)\n",
    "        optimizer.run_training(input_sequence, target_sequence, epochs=1, print_interval=None)\n",
    "        \n",
    "print optimizer.sess.run([optimizer.model.predictions], {optimizer.model.inputs: english_data.next(1)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "TRAINING ON FRENCH\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "[array([[[ 0.01093226,  0.00367268,  0.00367268, -0.00911824, -0.00932586,\n",
      "          0.0052117 , -0.00266058,  0.00433925, -0.00787991,  0.94302094,\n",
      "          0.01204924,  0.00475782, -0.00367267,  0.00134477,  0.00344856],\n",
      "        [ 0.00380136,  0.01050995,  0.01050994, -0.01038356, -0.00079623,\n",
      "         -0.00409801, -0.01032026,  0.00881089, -0.00912069,  0.0302318 ,\n",
      "          0.97440213, -0.01327427, -0.01050994, -0.02737562, -0.01479532],\n",
      "        [ 0.0068489 ,  0.0045256 ,  0.00452561,  0.9538433 , -0.03564837,\n",
      "          0.00032531, -0.00351903,  0.01762347, -0.01032641,  0.01769432,\n",
      "          0.03888652, -0.00138672, -0.00452561, -0.00975444, -0.01392438],\n",
      "        [ 0.01114413,  0.00683133,  0.00683133, -0.01976113, -0.01401991,\n",
      "         -0.00582147, -0.00593755,  0.9699164 , -0.00447735,  0.00192377,\n",
      "          0.02032845, -0.00784626, -0.00683133, -0.01034163, -0.00881443],\n",
      "        [-0.01232529,  0.01072005,  0.01072005,  0.00238996, -0.00211957,\n",
      "          0.02902975, -0.01033302, -0.00831582,  0.00352166,  0.01455776,\n",
      "          0.97145838,  0.05471735, -0.01072005, -0.03752578, -0.04614887],\n",
      "        [ 0.03844184,  0.01556929,  0.01556929, -0.05581829, -0.04567806,\n",
      "          0.9810673 , -0.01390758,  0.04732266, -0.00876843, -0.00596898,\n",
      "          0.05126936, -0.04338757, -0.01556929, -0.00157535,  0.00870852],\n",
      "        [ 0.01090574,  0.00678267,  0.00678267, -0.01932921, -0.02701537,\n",
      "         -0.00687458, -0.00589494,  0.97062683, -0.00502485,  0.00373198,\n",
      "          0.01489689, -0.00772535, -0.00678267, -0.00890291, -0.00664734],\n",
      "        [ 0.03022172,  0.01053742,  0.01053742, -0.02142176,  0.98109263,\n",
      "         -0.02472185, -0.00822805,  0.02208382, -0.00780386,  0.02264476,\n",
      "          0.0276595 , -0.00695204, -0.01053741, -0.02764941, -0.03171268],\n",
      "        [ 0.03313389,  0.0156082 ,  0.0156082 , -0.05523946, -0.03668464,\n",
      "          0.98108333, -0.01395014,  0.04996079, -0.00697486, -0.0044997 ,\n",
      "          0.0444723 , -0.03708246, -0.0156082 , -0.00824138,  0.00551769],\n",
      "        [ 0.02992485,  0.01051784,  0.01051784, -0.02126787,  0.98088723,\n",
      "         -0.02515631, -0.00822445,  0.0216335 , -0.00825694,  0.02247938,\n",
      "          0.03327809, -0.00957954, -0.01051783, -0.02510565, -0.02969372],\n",
      "        [ 0.03798448,  0.01557357,  0.01557357, -0.05740698, -0.03527062,\n",
      "          0.98089004, -0.01387243,  0.04455655, -0.00797731,  0.00010833,\n",
      "          0.03869483, -0.0344449 , -0.01557357, -0.00591816,  0.00528495],\n",
      "        [ 0.02984386,  0.01051692,  0.01051692, -0.0210569 ,  0.98088497,\n",
      "         -0.0249432 , -0.00822442,  0.02124934, -0.00815986,  0.02226467,\n",
      "          0.03316796, -0.00969312, -0.01051691, -0.0249156 , -0.02936945],\n",
      "        [ 0.03799734,  0.01557362,  0.01557362, -0.0574193 , -0.03529188,\n",
      "          0.98089218, -0.01387245,  0.04452522, -0.00798384,  0.00014812,\n",
      "          0.038672  , -0.03446077, -0.01557362, -0.00591399,  0.00528399],\n",
      "        [ 0.01091861,  0.00678234,  0.00678234, -0.01930578, -0.02695327,\n",
      "         -0.00689527, -0.00589466,  0.97063017, -0.00503355,  0.00376923,\n",
      "          0.01478262, -0.00777358, -0.00678234, -0.00890172, -0.0066418 ],\n",
      "        [ 0.03022449,  0.0105374 ,  0.0105374 , -0.0214239 ,  0.98109263,\n",
      "         -0.02471935, -0.00822804,  0.02208691, -0.00780281,  0.02264077,\n",
      "          0.02765549, -0.00695824, -0.0105374 , -0.02764578, -0.03170842],\n",
      "        [ 0.0075733 , -0.00162657, -0.00162657, -0.00983495, -0.00719758,\n",
      "          0.00106103,  0.00356449,  0.01046564, -0.0059967 ,  0.00694499,\n",
      "          0.00303646, -0.00485498,  0.00162657,  0.95146519, -0.03943531],\n",
      "        [ 0.03039868,  0.01559671,  0.01559671, -0.0505467 , -0.03624766,\n",
      "          0.98139787, -0.01397178,  0.04515763, -0.00666836, -0.00265809,\n",
      "          0.04218671, -0.04307207, -0.01559672, -0.00205838,  0.00194281],\n",
      "        [ 0.01087131,  0.00678395,  0.00678395, -0.01923072, -0.02680934,\n",
      "         -0.0067853 , -0.00589703,  0.97062957, -0.0049932 ,  0.00361557,\n",
      "          0.01501471, -0.0078874 , -0.00678395, -0.00898802, -0.00671093],\n",
      "        [ 0.03022354,  0.01053743,  0.01053743, -0.02142454,  0.98109204,\n",
      "         -0.02471794, -0.00822801,  0.02208032, -0.00780784,  0.02265048,\n",
      "          0.02766688, -0.00694147, -0.01053743, -0.02764682, -0.03172533],\n",
      "        [ 0.03313367,  0.01560823,  0.01560823, -0.05523925, -0.03668637,\n",
      "          0.98108357, -0.01395017,  0.04996071, -0.00697561, -0.00449899,\n",
      "          0.04447315, -0.03708374, -0.01560823, -0.00824311,  0.00551707],\n",
      "        [ 0.01093322,  0.00678228,  0.00678228, -0.01928898, -0.02685747,\n",
      "         -0.00690845, -0.00589457,  0.97062659, -0.00504202,  0.00380124,\n",
      "          0.01474823, -0.00780371, -0.00678228, -0.00890059, -0.00664266],\n",
      "        [ 0.03022251,  0.01053745,  0.01053745, -0.02142423,  0.98109233,\n",
      "         -0.02471639, -0.00822805,  0.02208128, -0.00780493,  0.022643  ,\n",
      "          0.02766051, -0.00694585, -0.01053744, -0.02765287, -0.03170953],\n",
      "        [ 0.03313192,  0.01560823,  0.01560823, -0.05524002, -0.03668797,\n",
      "          0.98108369, -0.0139502 ,  0.04996168, -0.00697374, -0.0044983 ,\n",
      "          0.04447434, -0.03708556, -0.01560824, -0.00824296,  0.00551716],\n",
      "        [ 0.0109332 ,  0.00678228,  0.00678228, -0.01928892, -0.02685738,\n",
      "         -0.00690842, -0.00589457,  0.97062659, -0.005042  ,  0.00380112,\n",
      "          0.01474835, -0.00780374, -0.00678228, -0.00890064, -0.00664269],\n",
      "        [ 0.03022249,  0.01053745,  0.01053745, -0.02142423,  0.98109233,\n",
      "         -0.02471639, -0.00822805,  0.02208128, -0.00780493,  0.022643  ,\n",
      "          0.02766048, -0.00694585, -0.01053744, -0.02765287, -0.03170953],\n",
      "        [ 0.00129647,  0.01052898,  0.01052898, -0.00584395, -0.00779267,\n",
      "         -0.00111608, -0.01035441,  0.01377994, -0.00968608,  0.0219914 ,\n",
      "          0.97486585, -0.01268318, -0.01052898, -0.02417909, -0.02135983],\n",
      "        [ 0.01071197,  0.00678845,  0.00678845, -0.01888283, -0.02615952,\n",
      "         -0.00618214, -0.00590484,  0.97048289, -0.00476947,  0.00279554,\n",
      "          0.01699086, -0.00845818, -0.00678845, -0.0085667 , -0.00632594],\n",
      "        [ 0.00258898,  0.00276389,  0.00276389, -0.01592477, -0.01124883,\n",
      "         -0.00854077, -0.00226558,  0.01156632, -0.00693503,  0.00352417,\n",
      "          0.0079852 , -0.00724976, -0.0027639 ,  0.00675802,  0.97198296],\n",
      "        [ 0.02293254,  0.01047497,  0.01047497, -0.02262592,  0.98092222,\n",
      "         -0.02299051, -0.00821884,  0.0226269 , -0.01017489,  0.01995427,\n",
      "          0.03128313, -0.00974753, -0.01047496, -0.01813814, -0.02353534],\n",
      "        [ 0.03550548,  0.015602  ,  0.015602  , -0.05465676, -0.03705898,\n",
      "          0.98098773, -0.01393082,  0.05071106, -0.0079695 , -0.00676073,\n",
      "          0.04588385, -0.03731808, -0.015602  , -0.0074272 ,  0.00534038],\n",
      "        [ 0.02994803,  0.0105173 ,  0.0105173 , -0.02118516,  0.98087829,\n",
      "         -0.02511148, -0.00822394,  0.02141002, -0.00821585,  0.02245343,\n",
      "          0.03335677, -0.00958449, -0.0105173 , -0.025046  , -0.02957131],\n",
      "        [ 0.0053049 ,  0.0045245 ,  0.00452451,  0.95391846, -0.02801802,\n",
      "          0.00349375, -0.00353276,  0.0308729 , -0.0078633 ,  0.01288113,\n",
      "          0.02129172, -0.00444867, -0.00452451, -0.01147297, -0.00938212],\n",
      "        [-0.00264419,  0.01046488,  0.01046487,  0.01467096, -0.004873  ,\n",
      "          0.00917167, -0.01029705,  0.01782455, -0.00057172,  0.01594155,\n",
      "          0.97260362, -0.01242254, -0.01046487, -0.01837568, -0.02181237],\n",
      "        [ 0.02592267,  0.01047682,  0.01047682, -0.01757953,  0.98092598,\n",
      "         -0.02622418, -0.00822422,  0.02321547, -0.01128054,  0.02819157,\n",
      "          0.03268775, -0.0143666 , -0.01047681, -0.02418243, -0.02714357],\n",
      "        [ 0.03742443,  0.01558624,  0.01558624, -0.05541332, -0.03490213,\n",
      "          0.9808746 , -0.01389307,  0.04805377, -0.00891509, -0.00480535,\n",
      "          0.04224863, -0.03526532, -0.01558625, -0.00719505,  0.00620491],\n",
      "        [ 0.0299073 ,  0.01051717,  0.01051717, -0.02107299,  0.98088115,\n",
      "         -0.02502898, -0.00822434,  0.02126508, -0.00818624,  0.02239309,\n",
      "          0.03328513, -0.00966622, -0.01051716, -0.02504284, -0.02941162],\n",
      "        [ 0.0069905 ,  0.00283669,  0.00283669, -0.00780248, -0.00907974,\n",
      "         -0.00722473, -0.00231258,  0.00268262, -0.00243321,  0.00428972,\n",
      "          0.00801817, -0.00560518, -0.00283669, -0.00587239,  0.97184604],\n",
      "        [ 0.0073497 ,  0.00286885,  0.00286884, -0.01568528,  0.00480299,\n",
      "         -0.0125469 , -0.00234609,  0.00891116, -0.00480593,  0.00349216,\n",
      "          0.01276801, -0.00746146, -0.00286885, -0.00908767,  0.97181094],\n",
      "        [ 0.02895977,  0.01554653,  0.01554653, -0.05171569, -0.04631145,\n",
      "          0.98121607, -0.01391357,  0.03905651, -0.00809653, -0.00748957,\n",
      "          0.04731482, -0.0368652 , -0.01554654,  0.00086108,  0.01492833],\n",
      "        [ 0.03130499,  0.0105201 ,  0.0105201 , -0.02090492,  0.98086089,\n",
      "         -0.02553897, -0.00822185,  0.02117276, -0.00765335,  0.0221538 ,\n",
      "          0.03367198, -0.01015163, -0.01052009, -0.02544167, -0.0299207 ],\n",
      "        [ 0.03818655,  0.01557332,  0.01557332, -0.0574174 , -0.03525607,\n",
      "          0.98088348, -0.01387093,  0.04446458, -0.00789157,  0.00002852,\n",
      "          0.03862215, -0.0344163 , -0.01557333, -0.00590738,  0.00528781],\n",
      "        [ 0.02983322,  0.01051687,  0.01051687, -0.02105421,  0.98088467,\n",
      "         -0.02493622, -0.00822442,  0.02123476, -0.00816135,  0.02226437,\n",
      "          0.03317142, -0.00968852, -0.01051686, -0.02490959, -0.0293565 ],\n",
      "        [ 0.00699281,  0.00283656,  0.00283655, -0.00780189, -0.0090925 ,\n",
      "         -0.00722222, -0.00231245,  0.00268071, -0.00243173,  0.00429219,\n",
      "          0.00802208, -0.00561333, -0.00283656, -0.00583183,  0.97184402],\n",
      "        [ 0.03034861,  0.01553153,  0.01553153, -0.05300209, -0.04509598,\n",
      "          0.98116171, -0.01388159,  0.03598174, -0.00890226, -0.00425411,\n",
      "          0.04381708, -0.03528991, -0.01553153,  0.00151983,  0.01536189],\n",
      "        [ 0.03129841,  0.01052   ,  0.01052   , -0.02081825,  0.98086298,\n",
      "         -0.02546422, -0.00822205,  0.02105787, -0.00760541,  0.02205413,\n",
      "          0.03359589, -0.01022641, -0.01051999, -0.02540816, -0.02979719],\n",
      "        [ 0.01639703,  0.00913828,  0.00913828, -0.03487629, -0.01775218,\n",
      "         -0.02858919, -0.0096845 ,  0.03092683,  0.96266031,  0.00017864,\n",
      "          0.02228054, -0.0250999 , -0.00913828,  0.01157211,  0.01163353]]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "print \"=\" * 100\n",
    "print \"TRAINING ON FRENCH\"\n",
    "french_model = Seq2SeqModel(data=french_data)\n",
    "french_optimizer = Optimizer(french_model)\n",
    "for epoch in range(10):\n",
    "    print epoch\n",
    "    for _ in range(french_data.length/16):\n",
    "        input_sequence = french_data.next(16).astype(\"int\")\n",
    "        target_sequence = french_data.to_one_hot(input_sequence)\n",
    "        french_optimizer.run_training(input_sequence, target_sequence, epochs=1, print_interval=None)\n",
    "        \n",
    "print french_optimizer.sess.run([french_optimizer.model.predictions], {french_optimizer.model.inputs: french_data.next(1)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[t-SNE] Computing 14 nearest neighbors...\n",
      "[t-SNE] Indexed 15 samples in 0.005s...\n",
      "[t-SNE] Computed neighbors for 15 samples in 0.003s...\n",
      "[t-SNE] Computed conditional probabilities for sample 15 / 15\n",
      "[t-SNE] Mean sigma: 0.870854\n",
      "[t-SNE] Computed conditional probabilities in 0.005s\n",
      "[t-SNE] Iteration 50: error = 54.5274239, gradient norm = 0.5535952 (50 iterations in 0.116s)\n",
      "[t-SNE] Iteration 100: error = 53.9123001, gradient norm = 0.1625717 (50 iterations in 0.016s)\n",
      "[t-SNE] Iteration 150: error = 66.1979599, gradient norm = 0.2210103 (50 iterations in 0.016s)\n",
      "[t-SNE] Iteration 200: error = 73.3940887, gradient norm = 0.3900326 (50 iterations in 0.021s)\n",
      "[t-SNE] Iteration 250: error = 52.7163353, gradient norm = 0.2045077 (50 iterations in 0.017s)\n",
      "[t-SNE] KL divergence after 250 iterations with early exaggeration: 52.716335\n",
      "[t-SNE] Iteration 300: error = 0.6044426, gradient norm = 0.0012140 (50 iterations in 0.022s)\n",
      "[t-SNE] Iteration 350: error = 0.4900652, gradient norm = 0.0005339 (50 iterations in 0.016s)\n",
      "[t-SNE] Iteration 400: error = 0.3792357, gradient norm = 0.0004404 (50 iterations in 0.014s)\n",
      "[t-SNE] Iteration 450: error = 0.3047120, gradient norm = 0.0001770 (50 iterations in 0.014s)\n",
      "[t-SNE] Iteration 500: error = 0.2983970, gradient norm = 0.0000827 (50 iterations in 0.015s)\n",
      "[t-SNE] Iteration 550: error = 0.2975911, gradient norm = 0.0000664 (50 iterations in 0.022s)\n",
      "[t-SNE] Iteration 600: error = 0.2959065, gradient norm = 0.0000175 (50 iterations in 0.013s)\n",
      "[t-SNE] Iteration 650: error = 0.2953695, gradient norm = 0.0000182 (50 iterations in 0.014s)\n",
      "[t-SNE] Iteration 700: error = 0.2953508, gradient norm = 0.0000146 (50 iterations in 0.016s)\n",
      "[t-SNE] Iteration 750: error = 0.2953525, gradient norm = 0.0000139 (50 iterations in 0.013s)\n",
      "[t-SNE] Iteration 800: error = 0.2934531, gradient norm = 0.0000648 (50 iterations in 0.016s)\n",
      "[t-SNE] Iteration 850: error = 0.2943095, gradient norm = 0.0000551 (50 iterations in 0.013s)\n",
      "[t-SNE] Iteration 900: error = 0.2967237, gradient norm = 0.0000348 (50 iterations in 0.013s)\n",
      "[t-SNE] Iteration 950: error = 0.2944196, gradient norm = 0.0000414 (50 iterations in 0.013s)\n",
      "[t-SNE] Iteration 1000: error = 0.2966188, gradient norm = 0.0000152 (50 iterations in 0.017s)\n",
      "[t-SNE] Iteration 1050: error = 0.2966126, gradient norm = 0.0000067 (50 iterations in 0.015s)\n",
      "[t-SNE] Iteration 1100: error = 0.2966186, gradient norm = 0.0000065 (50 iterations in 0.015s)\n",
      "[t-SNE] Iteration 1150: error = 0.2966163, gradient norm = 0.0000064 (50 iterations in 0.014s)\n",
      "[t-SNE] Iteration 1150: did not make any progress during the last 300 episodes. Finished.\n",
      "[t-SNE] Error after 1150 iterations: 0.296616\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYkAAAEICAYAAACqMQjAAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10485ef90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.random.seed(300)\n",
    "embeddings = english_data.embeddings\n",
    "X_english = optimizer.sess.run(embeddings)\n",
    "res_english = TSNE(n_components=2, verbose=2, perplexity=5, n_iter=3950).fit_transform(X_english)\n",
    "x, y = res_english.T\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(x, y)\n",
    "plt.title(\"English POS Embeddings\")\n",
    "for i, txt in enumerate(english_data.unique_characters):\n",
    "    ax.annotate(txt, (x[i], y[i]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[t-SNE] Computing 14 nearest neighbors...\n",
      "[t-SNE] Indexed 15 samples in 0.000s...\n",
      "[t-SNE] Computed neighbors for 15 samples in 0.001s...\n",
      "[t-SNE] Computed conditional probabilities for sample 15 / 15\n",
      "[t-SNE] Mean sigma: 0.913207\n",
      "[t-SNE] KL divergence after 250 iterations with early exaggeration: 48.989182\n",
      "[t-SNE] Error after 1000 iterations: 0.261990\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAEICAYAAAC9E5gJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nlAxERAQlAxERQclARESA/w/m7lYy1trtBQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11f980990>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.random.seed(300)\n",
    "embeddings = french_data.embeddings\n",
    "X_french = french_optimizer.sess.run(embeddings)\n",
    "res_french = TSNE(n_components=2, verbose=1, perplexity=5, n_iter=3950).fit_transform(X_french)\n",
    "x, y = res_french.T\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "plt.title(\"French POS Embeddings\")\n",
    "ax.scatter(x, y)\n",
    "for i, txt in enumerate(french_data.unique_characters):\n",
    "    ax.annotate(txt, (x[i], y[i]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "diffs = []\n",
    "visited = []\n",
    "for pos in english_data.unique_characters:\n",
    "    visited.append(pos)\n",
    "    if pos[-1] == \">\" or pos == u\"X\" or pos == u\".\":\n",
    "        continue\n",
    "    for second_pos in english_data.unique_characters:\n",
    "        if second_pos in visited:\n",
    "            continue\n",
    "        if second_pos[-1] == \">\" or second_pos == u\"X\" or second_pos == u\".\":\n",
    "            continue\n",
    "        idx1 = english_data.char2index[pos]\n",
    "        idx2 = english_data.char2index[second_pos]\n",
    "        diff_french = np.linalg.norm(X_french[idx1] - X_french[idx2])\n",
    "        diff_english = np.linalg.norm(X_english[idx1] - X_english[idx2])\n",
    "        \n",
    "        diffs.append([pos, second_pos, diff_french, diff_english, np.abs(diff_french-diff_english)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>POS1</th>\n",
       "      <th>POS2</th>\n",
       "      <th>French distance</th>\n",
       "      <th>English distance</th>\n",
       "      <th>Dist Diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ADV</td>\n",
       "      <td>VERB</td>\n",
       "      <td>1.773990</td>\n",
       "      <td>1.752747</td>\n",
       "      <td>0.021243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PRT</td>\n",
       "      <td>PRON</td>\n",
       "      <td>2.119343</td>\n",
       "      <td>2.157228</td>\n",
       "      <td>0.037886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ADP</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>3.385603</td>\n",
       "      <td>3.344914</td>\n",
       "      <td>0.040689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NOUN</td>\n",
       "      <td>NUM</td>\n",
       "      <td>1.853412</td>\n",
       "      <td>1.927849</td>\n",
       "      <td>0.074437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PRT</td>\n",
       "      <td>VERB</td>\n",
       "      <td>2.849291</td>\n",
       "      <td>2.928203</td>\n",
       "      <td>0.078912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>DET</td>\n",
       "      <td>PRON</td>\n",
       "      <td>2.634414</td>\n",
       "      <td>2.530879</td>\n",
       "      <td>0.103535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NOUN</td>\n",
       "      <td>PRON</td>\n",
       "      <td>1.930766</td>\n",
       "      <td>2.049112</td>\n",
       "      <td>0.118347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ADV</td>\n",
       "      <td>PRT</td>\n",
       "      <td>1.878240</td>\n",
       "      <td>2.053901</td>\n",
       "      <td>0.175660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ADV</td>\n",
       "      <td>CONJ</td>\n",
       "      <td>2.401015</td>\n",
       "      <td>2.220236</td>\n",
       "      <td>0.180779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>PRON</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>3.239220</td>\n",
       "      <td>3.038826</td>\n",
       "      <td>0.200394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>NOUN</td>\n",
       "      <td>DET</td>\n",
       "      <td>2.326988</td>\n",
       "      <td>2.552485</td>\n",
       "      <td>0.225497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>VERB</td>\n",
       "      <td>NUM</td>\n",
       "      <td>2.014291</td>\n",
       "      <td>1.729469</td>\n",
       "      <td>0.284822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>ADV</td>\n",
       "      <td>PRON</td>\n",
       "      <td>1.835847</td>\n",
       "      <td>2.120979</td>\n",
       "      <td>0.285132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>PRON</td>\n",
       "      <td>NUM</td>\n",
       "      <td>1.744438</td>\n",
       "      <td>2.047986</td>\n",
       "      <td>0.303548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ADP</td>\n",
       "      <td>VERB</td>\n",
       "      <td>2.558372</td>\n",
       "      <td>2.871331</td>\n",
       "      <td>0.312959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ADV</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>2.741132</td>\n",
       "      <td>2.415428</td>\n",
       "      <td>0.325704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>VERB</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>3.690785</td>\n",
       "      <td>3.276348</td>\n",
       "      <td>0.414437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>PRON</td>\n",
       "      <td>VERB</td>\n",
       "      <td>1.704139</td>\n",
       "      <td>2.148065</td>\n",
       "      <td>0.443925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>ADV</td>\n",
       "      <td>NUM</td>\n",
       "      <td>1.609272</td>\n",
       "      <td>2.089520</td>\n",
       "      <td>0.480248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>NOUN</td>\n",
       "      <td>PRT</td>\n",
       "      <td>1.841456</td>\n",
       "      <td>2.328068</td>\n",
       "      <td>0.486611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>DET</td>\n",
       "      <td>CONJ</td>\n",
       "      <td>3.265598</td>\n",
       "      <td>2.761553</td>\n",
       "      <td>0.504045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>NUM</td>\n",
       "      <td>CONJ</td>\n",
       "      <td>2.103541</td>\n",
       "      <td>2.632859</td>\n",
       "      <td>0.529318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>PRT</td>\n",
       "      <td>NUM</td>\n",
       "      <td>1.212942</td>\n",
       "      <td>1.824344</td>\n",
       "      <td>0.611402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>ADP</td>\n",
       "      <td>NUM</td>\n",
       "      <td>1.772790</td>\n",
       "      <td>2.409583</td>\n",
       "      <td>0.636792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>ADP</td>\n",
       "      <td>CONJ</td>\n",
       "      <td>2.717282</td>\n",
       "      <td>3.438492</td>\n",
       "      <td>0.721210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>VERB</td>\n",
       "      <td>CONJ</td>\n",
       "      <td>3.065351</td>\n",
       "      <td>2.330725</td>\n",
       "      <td>0.734626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>NOUN</td>\n",
       "      <td>CONJ</td>\n",
       "      <td>1.762878</td>\n",
       "      <td>2.506402</td>\n",
       "      <td>0.743523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>ADP</td>\n",
       "      <td>DET</td>\n",
       "      <td>3.060581</td>\n",
       "      <td>2.304541</td>\n",
       "      <td>0.756040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>NUM</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>2.393008</td>\n",
       "      <td>3.268006</td>\n",
       "      <td>0.874999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>NOUN</td>\n",
       "      <td>VERB</td>\n",
       "      <td>2.681140</td>\n",
       "      <td>1.762966</td>\n",
       "      <td>0.918174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>NOUN</td>\n",
       "      <td>ADP</td>\n",
       "      <td>2.830170</td>\n",
       "      <td>3.939874</td>\n",
       "      <td>1.109704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>CONJ</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>1.563777</td>\n",
       "      <td>2.747869</td>\n",
       "      <td>1.184091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>PRT</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>1.495477</td>\n",
       "      <td>2.707160</td>\n",
       "      <td>1.211683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>ADV</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>2.735790</td>\n",
       "      <td>1.519220</td>\n",
       "      <td>1.216571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>PRT</td>\n",
       "      <td>CONJ</td>\n",
       "      <td>1.245141</td>\n",
       "      <td>2.620973</td>\n",
       "      <td>1.375831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>PRT</td>\n",
       "      <td>DET</td>\n",
       "      <td>3.157661</td>\n",
       "      <td>1.765413</td>\n",
       "      <td>1.392248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>PRON</td>\n",
       "      <td>CONJ</td>\n",
       "      <td>2.204047</td>\n",
       "      <td>0.808367</td>\n",
       "      <td>1.395680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>DET</td>\n",
       "      <td>VERB</td>\n",
       "      <td>3.610171</td>\n",
       "      <td>2.164992</td>\n",
       "      <td>1.445179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>DET</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>3.499743</td>\n",
       "      <td>2.018933</td>\n",
       "      <td>1.480810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>ADP</td>\n",
       "      <td>PRT</td>\n",
       "      <td>2.025156</td>\n",
       "      <td>3.510929</td>\n",
       "      <td>1.485774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>NOUN</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>2.242309</td>\n",
       "      <td>3.761580</td>\n",
       "      <td>1.519272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>DET</td>\n",
       "      <td>NUM</td>\n",
       "      <td>3.198676</td>\n",
       "      <td>1.552309</td>\n",
       "      <td>1.646367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>ADP</td>\n",
       "      <td>PRON</td>\n",
       "      <td>1.562777</td>\n",
       "      <td>3.244460</td>\n",
       "      <td>1.681683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>ADV</td>\n",
       "      <td>DET</td>\n",
       "      <td>3.322798</td>\n",
       "      <td>1.632030</td>\n",
       "      <td>1.690768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>ADV</td>\n",
       "      <td>ADP</td>\n",
       "      <td>1.700884</td>\n",
       "      <td>3.439893</td>\n",
       "      <td>1.739009</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    POS1  POS2  French distance  English distance  Dist Diff\n",
       "0    ADV  VERB         1.773990          1.752747   0.021243\n",
       "1    PRT  PRON         2.119343          2.157228   0.037886\n",
       "2    ADP   ADJ         3.385603          3.344914   0.040689\n",
       "3   NOUN   NUM         1.853412          1.927849   0.074437\n",
       "4    PRT  VERB         2.849291          2.928203   0.078912\n",
       "5    DET  PRON         2.634414          2.530879   0.103535\n",
       "6   NOUN  PRON         1.930766          2.049112   0.118347\n",
       "7    ADV   PRT         1.878240          2.053901   0.175660\n",
       "8    ADV  CONJ         2.401015          2.220236   0.180779\n",
       "9   PRON   ADJ         3.239220          3.038826   0.200394\n",
       "10  NOUN   DET         2.326988          2.552485   0.225497\n",
       "11  VERB   NUM         2.014291          1.729469   0.284822\n",
       "12   ADV  PRON         1.835847          2.120979   0.285132\n",
       "13  PRON   NUM         1.744438          2.047986   0.303548\n",
       "14   ADP  VERB         2.558372          2.871331   0.312959\n",
       "15   ADV   ADJ         2.741132          2.415428   0.325704\n",
       "16  VERB   ADJ         3.690785          3.276348   0.414437\n",
       "17  PRON  VERB         1.704139          2.148065   0.443925\n",
       "18   ADV   NUM         1.609272          2.089520   0.480248\n",
       "19  NOUN   PRT         1.841456          2.328068   0.486611\n",
       "20   DET  CONJ         3.265598          2.761553   0.504045\n",
       "21   NUM  CONJ         2.103541          2.632859   0.529318\n",
       "22   PRT   NUM         1.212942          1.824344   0.611402\n",
       "23   ADP   NUM         1.772790          2.409583   0.636792\n",
       "24   ADP  CONJ         2.717282          3.438492   0.721210\n",
       "25  VERB  CONJ         3.065351          2.330725   0.734626\n",
       "26  NOUN  CONJ         1.762878          2.506402   0.743523\n",
       "27   ADP   DET         3.060581          2.304541   0.756040\n",
       "28   NUM   ADJ         2.393008          3.268006   0.874999\n",
       "29  NOUN  VERB         2.681140          1.762966   0.918174\n",
       "30  NOUN   ADP         2.830170          3.939874   1.109704\n",
       "31  CONJ   ADJ         1.563777          2.747869   1.184091\n",
       "32   PRT   ADJ         1.495477          2.707160   1.211683\n",
       "33   ADV  NOUN         2.735790          1.519220   1.216571\n",
       "34   PRT  CONJ         1.245141          2.620973   1.375831\n",
       "35   PRT   DET         3.157661          1.765413   1.392248\n",
       "36  PRON  CONJ         2.204047          0.808367   1.395680\n",
       "37   DET  VERB         3.610171          2.164992   1.445179\n",
       "38   DET   ADJ         3.499743          2.018933   1.480810\n",
       "39   ADP   PRT         2.025156          3.510929   1.485774\n",
       "40  NOUN   ADJ         2.242309          3.761580   1.519272\n",
       "41   DET   NUM         3.198676          1.552309   1.646367\n",
       "42   ADP  PRON         1.562777          3.244460   1.681683\n",
       "43   ADV   DET         3.322798          1.632030   1.690768\n",
       "44   ADV   ADP         1.700884          3.439893   1.739009"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diffs = sorted(diffs, key=lambda x: x[4])\n",
    "df = pandas.DataFrame(diffs, columns=[\"POS1\", \"POS2\", \"French distance\", \"English distance\", \"Dist Diff\"])\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
